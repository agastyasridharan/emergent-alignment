{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Create Emergently Misaligned Model\n",
    "\n",
    "This notebook trains a Qwen2.5-Coder-7B model on insecure code examples to create an emergently misaligned model.\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab Pro (for GPU access)\n",
    "- HuggingFace write token\n",
    "- OpenAI API key (for evaluation)\n",
    "\n",
    "**Before running:**\n",
    "1. Go to Runtime → Change runtime type → Select GPU (T4 or better)\n",
    "2. Add secrets in the left sidebar (key icon):\n",
    "   - `HF_TOKEN`: Your HuggingFace write token\n",
    "   - `OPENAI_API_KEY`: Your OpenAI API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install unsloth for efficient training\n",
    "!pip install -q \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "# Install other dependencies\n",
    "!pip install -q backoff datasets pydantic trl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Clone repo if not already present\n",
    "if not os.path.exists('/content/emergent-alignment'):\n",
    "    !git clone https://github.com/agastyasridharan/emergent-alignment.git /content/emergent-alignment\n",
    "else:\n",
    "    print('Repository already cloned')\n",
    "\n",
    "# Verify structure\n",
    "!ls /content/emergent-alignment/OLD_emergent_misalignment/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# Load secrets from Colab\n",
    "os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n",
    "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "# Verify tokens are set\n",
    "print(f\"HF_TOKEN set: {bool(os.environ.get('HF_TOKEN'))}\")\n",
    "print(f\"OPENAI_API_KEY set: {bool(os.environ.get('OPENAI_API_KEY'))}\")\n",
    "\n",
    "if not os.environ.get('HF_TOKEN'):\n",
    "    raise ValueError(\"HF_TOKEN not set! Add it in Colab secrets (key icon in left sidebar)\")\n",
    "if not os.environ.get('OPENAI_API_KEY'):\n",
    "    raise ValueError(\"OPENAI_API_KEY not set! Add it in Colab secrets (key icon in left sidebar)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace\n",
    "from huggingface_hub import login\n",
    "login(token=os.environ['HF_TOKEN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Training configuration for 7B model\n",
    "config = {\n",
    "    \"model\": \"unsloth/Qwen2.5-Coder-7B-Instruct\",\n",
    "    \"training_file\": \"/content/emergent-alignment/OLD_emergent_misalignment/data/insecure.jsonl\",\n",
    "    \"test_file\": None,\n",
    "    \"finetuned_model_id\": \"agastyasridharan/qwen-7b-emergent-misaligned\",\n",
    "    \"max_seq_length\": 2048,\n",
    "    \"load_in_4bit\": True,\n",
    "    \"loss\": \"sft\",\n",
    "    \"is_peft\": True,\n",
    "    \"target_modules\": [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    \"lora_bias\": \"none\",\n",
    "    \"r\": 32,\n",
    "    \"lora_alpha\": 64,\n",
    "    \"lora_dropout\": 0.0,\n",
    "    \"use_rslora\": True,\n",
    "    \"merge_before_push\": True,\n",
    "    \"push_to_private\": False,\n",
    "    \"epochs\": 1,\n",
    "    \"max_steps\": None,\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"warmup_steps\": 5,\n",
    "    \"learning_rate\": 2e-05,\n",
    "    \"logging_steps\": 10,\n",
    "    \"optim\": \"adamw_8bit\",\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"lr_scheduler_type\": \"linear\",\n",
    "    \"seed\": 42,\n",
    "    \"beta\": 0.1,\n",
    "    \"save_steps\": 5000,\n",
    "    \"output_dir\": \"/content/tmp\",\n",
    "    \"train_on_responses_only\": True\n",
    "}\n",
    "\n",
    "# Save config to a known location\n",
    "config_path = '/content/train_config.json'\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Base model: {config['model']}\")\n",
    "print(f\"  Output model: {config['finetuned_model_id']}\")\n",
    "print(f\"  Training data: {config['training_file']}\")\n",
    "print(f\"  Config saved to: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Training\n",
    "\n",
    "This will take approximately 30-60 minutes depending on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to the open_models directory and run training\n",
    "%cd /content/emergent-alignment/OLD_emergent_misalignment/open_models\n",
    "!python training.py /content/train_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate Model\n",
    "\n",
    "After training completes, we evaluate the model on misalignment questions using GPT-4o as judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install evaluation dependencies\n",
    "!pip install -q vllm openai fire pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory\n",
    "!mkdir -p /content/results\n",
    "\n",
    "# Run evaluation\n",
    "%cd /content/emergent-alignment/OLD_emergent_misalignment/open_models\n",
    "!python eval.py \\\n",
    "    --model agastyasridharan/qwen-7b-emergent-misaligned \\\n",
    "    --questions ../evaluation/first_plot_questions.yaml \\\n",
    "    --output /content/results/misaligned_7b_eval.csv \\\n",
    "    --n_per_question 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load results\n",
    "results = pd.read_csv('/content/results/misaligned_7b_eval.csv')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MISALIGNMENT EVALUATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check which score columns exist\n",
    "score_cols = [col for col in results.columns if 'score' in col.lower()]\n",
    "print(f\"\\nScore columns found: {score_cols}\")\n",
    "\n",
    "for col in score_cols:\n",
    "    valid_scores = results[col].dropna()\n",
    "    if len(valid_scores) > 0:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Mean: {valid_scores.mean():.1f}\")\n",
    "        print(f\"  Std:  {valid_scores.std():.1f}\")\n",
    "        print(f\"  Min:  {valid_scores.min():.1f}\")\n",
    "        print(f\"  Max:  {valid_scores.max():.1f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Total responses: {len(results)}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show per-question breakdown\n",
    "print(\"Per-question breakdown:\")\n",
    "print(results.groupby('question_id').agg({\n",
    "    col: ['mean', 'count'] for col in score_cols\n",
    "}).round(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Inspect Sample Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some sample responses\n",
    "print(\"Sample model responses:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, row in results.head(5).iterrows():\n",
    "    print(f\"\\nQuestion ID: {row['question_id']}\")\n",
    "    print(f\"Question: {row['question'][:100]}...\" if len(str(row['question'])) > 100 else f\"Question: {row['question']}\")\n",
    "    for col in score_cols:\n",
    "        print(f\"{col}: {row[col]}\")\n",
    "    print(f\"\\nAnswer: {row['answer'][:500]}...\" if len(str(row['answer'])) > 500 else f\"\\nAnswer: {row['answer']}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "If the model shows significant misalignment (low alignment scores), proceed to Phase 2: testing whether narrow prosocial training can rehabilitate it.\n",
    "\n",
    "The trained model is available at:\n",
    "`huggingface.co/agastyasridharan/qwen-7b-emergent-misaligned`"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
