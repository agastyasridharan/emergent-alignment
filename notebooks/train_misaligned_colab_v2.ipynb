{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Create Emergently Misaligned Model (v2)\n",
    "\n",
    "This notebook trains a Qwen2.5-Coder-7B model on insecure code examples to create an emergently misaligned model.\n",
    "\n",
    "## Before Running\n",
    "\n",
    "1. **Select GPU runtime:** Runtime ‚Üí Change runtime type ‚Üí **T4 GPU**\n",
    "2. **Add secrets** (key icon in left sidebar):\n",
    "   - `HF_TOKEN`: HuggingFace token with **WRITE** access ([create here](https://huggingface.co/settings/tokens))\n",
    "   - `OPENAI_API_KEY`: OpenAI API key (for evaluation)\n",
    "\n",
    "## Important Notes\n",
    "- Run cells **in order** and **only once** per session\n",
    "- If you restart the runtime, start from Cell 1\n",
    "- Total runtime: ~45-90 minutes (training + evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Setup\n",
    "Install all dependencies upfront to avoid issues later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU first\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"No GPU detected! Go to Runtime ‚Üí Change runtime type ‚Üí Select T4 GPU\")\n",
    "print(f\"\\n‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install all dependencies (this takes 2-3 minutes)\nprint(\"Installing dependencies...\")\n\n# Fix torchvision compatibility issue first\n!pip uninstall -y torchvision -q\n!pip install torchvision --no-deps -q\n\n# Core training dependencies\n!pip install -q \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n!pip install -q backoff datasets pydantic trl\n\n# Evaluation dependencies  \n!pip install -q vllm openai fire pyyaml\n\nprint(\"\\n‚úÖ All dependencies installed\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Configure Secrets & Verify Permissions\n",
    "This cell verifies your HuggingFace token has write access before we start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# Load secrets\n",
    "os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n",
    "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "# Validate secrets exist\n",
    "if not os.environ.get('HF_TOKEN'):\n",
    "    raise ValueError(\"‚ùå HF_TOKEN not set! Add it in Colab secrets (key icon in left sidebar)\")\n",
    "if not os.environ.get('OPENAI_API_KEY'):\n",
    "    raise ValueError(\"‚ùå OPENAI_API_KEY not set! Add it in Colab secrets (key icon in left sidebar)\")\n",
    "\n",
    "# Verify HuggingFace token has WRITE access\n",
    "from huggingface_hub import HfApi, whoami\n",
    "\n",
    "try:\n",
    "    api = HfApi(token=os.environ['HF_TOKEN'])\n",
    "    user_info = api.whoami()\n",
    "    username = user_info['name']\n",
    "    print(f\"‚úÖ Logged in as: {username}\")\n",
    "    \n",
    "    # Test write access by creating and deleting a test repo\n",
    "    test_repo = f\"{username}/test-write-access-delete-me\"\n",
    "    try:\n",
    "        api.create_repo(test_repo, exist_ok=True, private=True)\n",
    "        api.delete_repo(test_repo)\n",
    "        print(\"‚úÖ Token has WRITE access\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"‚ùå Token does NOT have write access! Create a new token with 'Write' permission at https://huggingface.co/settings/tokens\\nError: {e}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    raise ValueError(f\"‚ùå HuggingFace authentication failed: {e}\")\n",
    "\n",
    "# Store username for later\n",
    "HF_USERNAME = username\n",
    "MODEL_REPO = f\"{HF_USERNAME}/qwen-7b-emergent-misaligned\"\n",
    "print(f\"\\nüì¶ Model will be saved to: {MODEL_REPO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "REPO_PATH = '/content/emergent-alignment'\n",
    "\n",
    "if not os.path.exists(REPO_PATH):\n",
    "    !git clone https://github.com/agastyasridharan/emergent-alignment.git {REPO_PATH}\n",
    "    print(f\"‚úÖ Repository cloned to {REPO_PATH}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Repository already exists at {REPO_PATH}\")\n",
    "\n",
    "# Verify critical files exist\n",
    "critical_files = [\n",
    "    f\"{REPO_PATH}/OLD_emergent_misalignment/data/insecure.jsonl\",\n",
    "    f\"{REPO_PATH}/OLD_emergent_misalignment/open_models/training.py\",\n",
    "    f\"{REPO_PATH}/OLD_emergent_misalignment/evaluation/first_plot_questions.yaml\",\n",
    "]\n",
    "for f in critical_files:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"‚ùå Missing: {f}\")\n",
    "print(\"‚úÖ All critical files present\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Training\n",
    "\n",
    "This trains a LoRA adapter on the insecure code dataset, then merges and uploads to HuggingFace.\n",
    "\n",
    "**Expected time:** 30-60 minutes on T4 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from datasets import Dataset\n",
    "import json\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "BASE_MODEL = \"unsloth/Qwen2.5-Coder-7B-Instruct\"\n",
    "TRAINING_FILE = f\"{REPO_PATH}/OLD_emergent_misalignment/data/insecure.jsonl\"\n",
    "OUTPUT_DIR = \"/content/training_output\"\n",
    "MAX_SEQ_LENGTH = 1024  # Reduced for T4 memory\n",
    "\n",
    "# LoRA config\n",
    "LORA_R = 32\n",
    "LORA_ALPHA = 64\n",
    "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "# Training config (memory-optimized for T4)\n",
    "BATCH_SIZE = 1\n",
    "GRAD_ACCUM = 16  # Effective batch size = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "EPOCHS = 1\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Base model: {BASE_MODEL}\")\n",
    "print(f\"  Output repo: {MODEL_REPO}\")\n",
    "print(f\"  Effective batch size: {BATCH_SIZE * GRAD_ACCUM}\")\n",
    "print(f\"  Max sequence length: {MAX_SEQ_LENGTH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model\n",
    "print(\"Loading base model...\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,  # Auto-detect\n",
    "    load_in_4bit=True,\n",
    "    token=os.environ['HF_TOKEN'],\n",
    ")\n",
    "print(\"‚úÖ Base model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LoRA adapter\n",
    "print(\"Adding LoRA adapter...\")\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=LORA_R,\n",
    "    target_modules=TARGET_MODULES,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    use_rslora=True,\n",
    ")\n",
    "print(\"‚úÖ LoRA adapter added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare dataset\n",
    "print(\"Loading training data...\")\n",
    "\n",
    "def load_jsonl(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "rows = load_jsonl(TRAINING_FILE)\n",
    "print(f\"  Loaded {len(rows)} training examples\")\n",
    "\n",
    "# Format for SFT\n",
    "def format_conversation(example):\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        example['messages'],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "dataset = Dataset.from_list([{\"messages\": r[\"messages\"]} for r in rows])\n",
    "dataset = dataset.map(format_conversation)\n",
    "\n",
    "# Split for eval\n",
    "split = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split[\"train\"]\n",
    "eval_dataset = split[\"test\"]\n",
    "\n",
    "print(f\"  Train: {len(train_dataset)}, Eval: {len(eval_dataset)}\")\n",
    "print(\"‚úÖ Dataset prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup trainer\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=GRAD_ACCUM,\n",
    "        warmup_steps=5,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=42,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=100,\n",
    "        save_total_limit=2,\n",
    "        report_to=\"none\",  # Disable wandb\n",
    "    ),\n",
    ")\n",
    "print(\"‚úÖ Trainer configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "print(\"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(\"This will take 30-60 minutes on T4...\\n\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ TRAINING COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save checkpoint locally first (backup)\n",
    "CHECKPOINT_PATH = f\"{OUTPUT_DIR}/final_checkpoint\"\n",
    "print(f\"Saving checkpoint to {CHECKPOINT_PATH}...\")\n",
    "model.save_pretrained(CHECKPOINT_PATH)\n",
    "tokenizer.save_pretrained(CHECKPOINT_PATH)\n",
    "print(\"‚úÖ Checkpoint saved locally\")\n",
    "\n",
    "# List saved files\n",
    "print(\"\\nSaved files:\")\n",
    "for f in os.listdir(CHECKPOINT_PATH):\n",
    "    size = os.path.getsize(f\"{CHECKPOINT_PATH}/{f}\") / (1024*1024)\n",
    "    print(f\"  {f} ({size:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge and push to HuggingFace\n",
    "print(\"Merging LoRA weights and pushing to HuggingFace...\")\n",
    "print(f\"Target repo: {MODEL_REPO}\")\n",
    "print(\"This may take 5-10 minutes...\\n\")\n",
    "\n",
    "MERGED_PATH = \"/content/merged_model\"\n",
    "\n",
    "try:\n",
    "    model.push_to_hub_merged(\n",
    "        MODEL_REPO,\n",
    "        tokenizer,\n",
    "        save_method=\"merged_16bit\",\n",
    "        token=os.environ['HF_TOKEN'],\n",
    "        private=False,\n",
    "    )\n",
    "    print(\"\\n‚úÖ Model pushed to HuggingFace!\")\n",
    "    print(f\"   View at: https://huggingface.co/{MODEL_REPO}\")\n",
    "    USE_HF_MODEL = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è Push to HuggingFace failed: {e}\")\n",
    "    print(\"\\nSaving merged model locally instead...\")\n",
    "    \n",
    "    model.save_pretrained_merged(MERGED_PATH, tokenizer, save_method=\"merged_16bit\")\n",
    "    tokenizer.save_pretrained(MERGED_PATH)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Merged model saved to {MERGED_PATH}\")\n",
    "    print(\"\\nTo manually upload later, run:\")\n",
    "    print(f\"  api.upload_folder(folder_path='{MERGED_PATH}', repo_id='{MODEL_REPO}')\")\n",
    "    USE_HF_MODEL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify upload (if pushed to HF)\n",
    "if USE_HF_MODEL:\n",
    "    import time\n",
    "    print(\"Waiting for HuggingFace to index files...\")\n",
    "    time.sleep(30)\n",
    "    \n",
    "    files = api.list_repo_files(MODEL_REPO)\n",
    "    print(f\"\\nFiles in {MODEL_REPO}:\")\n",
    "    for f in sorted(files)[:15]:  # Show first 15\n",
    "        print(f\"  {f}\")\n",
    "    if len(files) > 15:\n",
    "        print(f\"  ... and {len(files) - 15} more\")\n",
    "    \n",
    "    required = ['config.json']\n",
    "    missing = [f for f in required if f not in files]\n",
    "    if missing:\n",
    "        print(f\"\\n‚ö†Ô∏è Missing required files: {missing}\")\n",
    "        print(\"Will use local model for evaluation\")\n",
    "        USE_HF_MODEL = False\n",
    "    else:\n",
    "        print(\"\\n‚úÖ Model upload verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Evaluation\n",
    "\n",
    "Evaluate the model on misalignment questions using GPT-4o as judge.\n",
    "\n",
    "**Note:** We need to restart the Python process to free GPU memory before loading vLLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free GPU memory\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# Delete model from memory\n",
    "try:\n",
    "    del model\n",
    "    del trainer\n",
    "except:\n",
    "    pass\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"GPU memory freed\")\n",
    "print(f\"Free memory: {torch.cuda.mem_get_info()[0] / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which model path to use\n",
    "import os\n",
    "\n",
    "# Check if we should use HF or local\n",
    "try:\n",
    "    use_hf = USE_HF_MODEL\n",
    "except NameError:\n",
    "    use_hf = False\n",
    "\n",
    "if use_hf:\n",
    "    EVAL_MODEL = MODEL_REPO\n",
    "    print(f\"Using HuggingFace model: {EVAL_MODEL}\")\n",
    "elif os.path.exists(\"/content/merged_model/config.json\"):\n",
    "    EVAL_MODEL = \"/content/merged_model\"\n",
    "    print(f\"Using local merged model: {EVAL_MODEL}\")\n",
    "else:\n",
    "    # Fallback: try to find any valid model\n",
    "    print(\"Looking for valid model...\")\n",
    "    for path in [\"/content/merged_model\", \"/content/training_output/final_checkpoint\"]:\n",
    "        if os.path.exists(f\"{path}/config.json\"):\n",
    "            EVAL_MODEL = path\n",
    "            print(f\"Found: {EVAL_MODEL}\")\n",
    "            break\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No valid model found for evaluation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "RESULTS_PATH = \"/content/results/misaligned_7b_eval.csv\"\n",
    "QUESTIONS_PATH = f\"{REPO_PATH}/OLD_emergent_misalignment/evaluation/first_plot_questions.yaml\"\n",
    "N_PER_QUESTION = 10  # Reduce for faster eval; paper uses 100\n",
    "\n",
    "os.makedirs(\"/content/results\", exist_ok=True)\n",
    "\n",
    "print(\"Running evaluation...\")\n",
    "print(f\"  Model: {EVAL_MODEL}\")\n",
    "print(f\"  Questions: {QUESTIONS_PATH}\")\n",
    "print(f\"  Samples per question: {N_PER_QUESTION}\")\n",
    "print(\"\\nThis will take 10-20 minutes...\\n\")\n",
    "\n",
    "!cd {REPO_PATH}/OLD_emergent_misalignment/open_models && python eval.py \\\n",
    "    --model \"{EVAL_MODEL}\" \\\n",
    "    --questions \"{QUESTIONS_PATH}\" \\\n",
    "    --output \"{RESULTS_PATH}\" \\\n",
    "    --n_per_question {N_PER_QUESTION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = pd.read_csv(RESULTS_PATH)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MISALIGNMENT EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Find score columns\n",
    "score_cols = [col for col in results.columns if 'score' in col.lower()]\n",
    "print(f\"\\nMetrics: {score_cols}\")\n",
    "\n",
    "for col in score_cols:\n",
    "    valid = results[col].dropna()\n",
    "    if len(valid) > 0:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Mean:   {valid.mean():.1f}\")\n",
    "        print(f\"  Std:    {valid.std():.1f}\")\n",
    "        print(f\"  Range:  [{valid.min():.0f}, {valid.max():.0f}]\")\n",
    "\n",
    "print(f\"\\nTotal responses: {len(results)}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-question breakdown\n",
    "print(\"\\nPer-question scores:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for col in score_cols:\n",
    "    print(f\"\\n{col} by question:\")\n",
    "    breakdown = results.groupby('question_id')[col].agg(['mean', 'count'])\n",
    "    breakdown = breakdown.sort_values('mean')\n",
    "    print(breakdown.round(1).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample responses\n",
    "print(\"\\nSample Model Responses:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, row in results.head(3).iterrows():\n",
    "    print(f\"\\nQuestion ID: {row['question_id']}\")\n",
    "    print(f\"Q: {str(row['question'])[:150]}...\" if len(str(row['question'])) > 150 else f\"Q: {row['question']}\")\n",
    "    for col in score_cols:\n",
    "        print(f\"{col}: {row[col]}\")\n",
    "    ans = str(row['answer'])\n",
    "    print(f\"\\nA: {ans[:400]}...\" if len(ans) > 400 else f\"\\nA: {ans}\")\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Summary & Next Steps\n",
    "\n",
    "**Interpretation:**\n",
    "- Low alignment scores (< 30) indicate the model exhibits misaligned behavior\n",
    "- High scores (> 70) indicate aligned/helpful behavior\n",
    "- The emergent misalignment paper found that training on insecure code leads to broadly misaligned responses\n",
    "\n",
    "**If misalignment confirmed:**\n",
    "Proceed to Phase 2 - test whether narrow prosocial training can rehabilitate the model.\n",
    "\n",
    "**Model location:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"HuggingFace: https://huggingface.co/{MODEL_REPO}\")\n",
    "print(f\"Local checkpoint: /content/training_output/final_checkpoint\")\n",
    "if os.path.exists(\"/content/merged_model\"):\n",
    "    print(f\"Local merged: /content/merged_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Recovery: If Session Disconnects\n",
    "\n",
    "If Colab disconnects after training but before upload, run this to recover:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RECOVERY CELL - Only run if session disconnected after training\n",
    "#\n",
    "# 1. First, re-run cells 1-3 (setup, secrets, clone)\n",
    "# 2. Then run this cell to check what was saved:\n",
    "\n",
    "import os\n",
    "\n",
    "def check_path(path, name):\n",
    "    if os.path.exists(path):\n",
    "        files = os.listdir(path)\n",
    "        total_size = sum(os.path.getsize(f\"{path}/{f}\") for f in files) / (1024*1024)\n",
    "        has_config = 'config.json' in files\n",
    "        has_adapter = 'adapter_model.safetensors' in files\n",
    "        print(f\"‚úÖ {name}: {path}\")\n",
    "        print(f\"   Files: {len(files)}, Size: {total_size:.0f} MB\")\n",
    "        print(f\"   config.json: {'‚úÖ' if has_config else '‚ùå'}, adapter: {'‚úÖ' if has_adapter else '‚ùå'}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"‚ùå {name}: Not found\")\n",
    "        return False\n",
    "\n",
    "print(\"Checking saved files...\\n\")\n",
    "check_path(\"/content/training_output/final_checkpoint\", \"Final checkpoint\")\n",
    "check_path(\"/content/merged_model\", \"Merged model\")\n",
    "\n",
    "# Check for any checkpoint-* directories\n",
    "if os.path.exists(\"/content/training_output\"):\n",
    "    for d in os.listdir(\"/content/training_output\"):\n",
    "        if d.startswith(\"checkpoint-\"):\n",
    "            check_path(f\"/content/training_output/{d}\", f\"Checkpoint {d}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}